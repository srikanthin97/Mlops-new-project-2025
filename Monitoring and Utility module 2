# ML Monitoring & Utilities Module
# Reusable components for model monitoring and operations

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
import json
import logging

# ============================================================================
# 1. MODEL DRIFT DETECTION
# ============================================================================

class DriftDetector:
    """Detects data and prediction drift"""
    
    def __init__(self, baseline_data: pd.DataFrame):
        self.baseline_data = baseline_data
        self.baseline_stats = self._compute_stats(baseline_data)
        self.logger = logging.getLogger(__name__)
    
    def _compute_stats(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Compute statistical properties of data"""
        stats = {}
        for col in data.select_dtypes(include=[np.number]).columns:
            stats[col] = {
                'mean': data[col].mean(),
                'std': data[col].std(),
                'min': data[col].min(),
                'max': data[col].max(),
                'quantiles': data[col].quantile([0.25, 0.5, 0.75]).to_dict()
            }
        return stats
    
    def detect_feature_drift(
        self, 
        current_data: pd.DataFrame,
        threshold: float = 0.1
    ) -> Dict[str, Any]:
        """
        Detect drift in feature distributions using PSI (Population Stability Index)
        """
        drift_results = {}
        current_stats = self._compute_stats(current_data)
        
        for feature in self.baseline_stats.keys():
            if feature not in current_stats:
                continue
            
            # Calculate PSI
            psi = self._calculate_psi(
                self.baseline_data[feature],
                current_data[feature]
            )
            
            drift_results[feature] = {
                'psi': psi,
                'has_drift': psi > threshold,
                'baseline_mean': self.baseline_stats[feature]['mean'],
                'current_mean': current_stats[feature]['mean'],
                'mean_shift': abs(current_stats[feature]['mean'] - 
                                self.baseline_stats[feature]['mean'])
            }
        
        return drift_results
    
    def _calculate_psi(
        self, 
        baseline: pd.Series, 
        current: pd.Series, 
        bins: int = 10
    ) -> float:
        """Calculate Population Stability Index"""
        try:
            # Create bins based on baseline
            breakpoints = np.quantile(baseline, np.linspace(0, 1, bins + 1))
            breakpoints = np.unique(breakpoints)
            
            # Count in each bin
            baseline_counts = np.histogram(baseline, bins=breakpoints)[0]
            current_counts = np.histogram(current, bins=breakpoints)[0]
            
            # Calculate percentages
            baseline_pct = baseline_counts / len(baseline)
            current_pct = current_counts / len(current)
            
            # Avoid division by zero
            baseline_pct = np.where(baseline_pct == 0, 0.0001, baseline_pct)
            current_pct = np.where(current_pct == 0, 0.0001, current_pct)
            
            # Calculate PSI
            psi = np.sum((current_pct - baseline_pct) * 
                        np.log(current_pct / baseline_pct))
            
            return float(psi)
        except Exception as e:
            self.logger.error(f"PSI calculation failed: {str(e)}")
            return 0.0
    
    def detect_prediction_drift(
        self,
        baseline_predictions: np.ndarray,
        current_predictions: np.ndarray,
        threshold: float = 0.1
    ) -> Dict[str, Any]:
        """Detect drift in model predictions"""
        
        # KL divergence for classification
        if len(np.unique(baseline_predictions)) < 20:
            drift_score = self._calculate_kl_divergence(
                baseline_predictions,
                current_predictions
            )
        else:
            # PSI for regression or high cardinality
            drift_score = self._calculate_psi(
                pd.Series(baseline_predictions),
                pd.Series(current_predictions)
            )
        
        return {
            'drift_score': drift_score,
            'has_drift': drift_score > threshold,
            'baseline_mean': np.mean(baseline_predictions),
            'current_mean': np.mean(current_predictions)
        }
    
    def _calculate_kl_divergence(
        self,
        p: np.ndarray,
        q: np.ndarray
    ) -> float:
        """Calculate Kullback-Leibler divergence"""
        from scipy.special import kl_div
        
        # Get probability distributions
        p_dist = np.bincount(p.astype(int)) / len(p)
        q_dist = np.bincount(q.astype(int)) / len(q)
        
        # Ensure same length
        max_len = max(len(p_dist), len(q_dist))
        p_dist = np.pad(p_dist, (0, max_len - len(p_dist)))
        q_dist = np.pad(q_dist, (0, max_len - len(q_dist)))
        
        # Avoid zero probabilities
        p_dist = np.where(p_dist == 0, 0.0001, p_dist)
        q_dist = np.where(q_dist == 0, 0.0001, q_dist)
        
        return float(np.sum(kl_div(p_dist, q_dist)))


# ============================================================================
# 2. PERFORMANCE METRICS TRACKER
# ============================================================================

class MetricsTracker:
    """Track and aggregate model performance metrics"""
    
    def __init__(self, storage_backend: str = "memory"):
        self.storage_backend = storage_backend
        self.metrics_history = []
        self.logger = logging.getLogger(__name__)
    
    def log_metrics(
        self,
        endpoint_id: str,
        metrics: Dict[str, float],
        timestamp: Optional[datetime] = None
    ):
        """Log metrics for an endpoint"""
        if timestamp is None:
            timestamp = datetime.now()
        
        metric_entry = {
            'endpoint_id': endpoint_id,
            'timestamp': timestamp,
            'metrics': metrics
        }
        
        self.metrics_history.append(metric_entry)
        self.logger.info(f"Logged metrics for {endpoint_id}: {metrics}")
    
    def get_metrics(
        self,
        endpoint_id: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None
    ) -> List[Dict[str, Any]]:
        """Retrieve metrics for an endpoint within time range"""
        filtered = [
            m for m in self.metrics_history
            if m['endpoint_id'] == endpoint_id
        ]
        
        if start_time:
            filtered = [m for m in filtered if m['timestamp'] >= start_time]
        if end_time:
            filtered = [m for m in filtered if m['timestamp'] <= end_time]
        
        return filtered
    
    def calculate_aggregates(
        self,
        endpoint_id: str,
        window: str = "1h"
    ) -> Dict[str, float]:
        """Calculate aggregate metrics over time window"""
        
        # Parse window
        window_td = self._parse_window(window)
        start_time = datetime.now() - window_td
        
        # Get metrics in window
        metrics = self.get_metrics(endpoint_id, start_time=start_time)
        
        if not metrics:
            return {}
        
        # Aggregate by metric name
        aggregates = {}
        all_metric_names = set()
        for m in metrics:
            all_metric_names.update(m['metrics'].keys())
        
        for metric_name in all_metric_names:
            values = [
                m['metrics'][metric_name]
                for m in metrics
                if metric_name in m['metrics']
            ]
            
            if values:
                aggregates[f"{metric_name}_avg"] = np.mean(values)
                aggregates[f"{metric_name}_p50"] = np.percentile(values, 50)
                aggregates[f"{metric_name}_p95"] = np.percentile(values, 95)
                aggregates[f"{metric_name}_p99"] = np.percentile(values, 99)
                aggregates[f"{metric_name}_max"] = np.max(values)
                aggregates[f"{metric_name}_min"] = np.min(values)
        
        return aggregates
    
    def _parse_window(self, window: str) -> timedelta:
        """Parse time window string (e.g., '1h', '24h', '7d')"""
        unit = window[-1]
        value = int(window[:-1])
        
        if unit == 'h':
            return timedelta(hours=value)
        elif unit == 'd':
            return timedelta(days=value)
        elif unit == 'm':
            return timedelta(minutes=value)
        else:
            return timedelta(hours=1)


# ============================================================================
# 3. ALERT MANAGER
# ============================================================================

@dataclass
class Alert:
    """Alert definition"""
    name: str
    metric_name: str
    condition: str  # '>', '<', '==', '>=', '<='
    threshold: float
    severity: str  # 'info', 'warning', 'critical'
    message: str
    triggered_at: datetime


class AlertManager:
    """Manage and trigger alerts based on metrics"""
    
    def __init__(self, notification_channels: Optional[List[str]] = None):
        self.notification_channels = notification_channels or []
        self.active_alerts = []
        self.alert_history = []
        self.logger = logging.getLogger(__name__)
    
    def check_alerts(
        self,
        metrics: Dict[str, float],
        alert_configs: List[Dict[str, Any]]
    ) -> List[Alert]:
        """Check if any alerts should be triggered"""
        triggered = []
        
        for config in alert_configs:
            metric_name = config['metric']
            
            if metric_name not in metrics:
                continue
            
            metric_value = metrics[metric_name]
            threshold = config['threshold']
            condition = config['condition']
            
            if self._evaluate_condition(metric_value, condition, threshold):
                alert = Alert(
                    name=config['name'],
                    metric_name=metric_name,
                    condition=condition,
                    threshold=threshold,
                    severity=config.get('severity', 'warning'),
                    message=f"{metric_name} {condition} {threshold} (current: {metric_value})",
                    triggered_at=datetime.now()
                )
                
                triggered.append(alert)
                self.active_alerts.append(alert)
                self.alert_history.append(alert)
                
                # Send notifications
                self._send_notifications(alert, config.get('channels', []))
        
        return triggered
    
    def _evaluate_condition(
        self,
        value: float,
        condition: str,
        threshold: float
    ) -> bool:
        """Evaluate alert condition"""
        if condition == '>':
            return value > threshold
        elif condition == '<':
            return value < threshold
        elif condition == '>=':
            return value >= threshold
        elif condition == '<=':
            return value <= threshold
        elif condition == '==':
            return abs(value - threshold) < 0.0001
        return False
    
    def _send_notifications(self, alert: Alert, channels: List[str]):
        """Send alert notifications"""
        for channel in channels:
            try:
                if channel == 'email':
                    self._send_email(alert)
                elif channel == 'slack':
                    self._send_slack(alert)
                elif channel == 'pagerduty':
                    self._send_pagerduty(alert)
                
                self.logger.info(f"Alert sent via {channel}: {alert.name}")
            except Exception as e:
                self.logger.error(f"Failed to send alert via {channel}: {str(e)}")
    
    def _send_email(self, alert: Alert):
        """Send email notification (placeholder)"""
        self.logger.info(f"EMAIL: {alert.severity.upper()} - {alert.message}")
    
    def _send_slack(self, alert: Alert):
        """Send Slack notification (placeholder)"""
        self.logger.info(f"SLACK: {alert.severity.upper()} - {alert.message}")
    
    def _send_pagerduty(self, alert: Alert):
        """Send PagerDuty notification (placeholder)"""
        self.logger.info(f"PAGERDUTY: {alert.severity.upper()} - {alert.message}")
    
    def clear_alert(self, alert_name: str):
        """Clear an active alert"""
        self.active_alerts = [
            a for a in self.active_alerts
            if a.name != alert_name
        ]


# ============================================================================
# 4. MODEL VERSIONING MANAGER
# ============================================================================

class ModelVersionManager:
    """Manage model versions and metadata"""
    
    def __init__(self, registry_path: str):
        self.registry_path = registry_path
        self.versions = {}
        self.logger = logging.getLogger(__name__)
    
    def register_model(
        self,
        model_name: str,
        version: str,
        model_path: str,
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Register a new model version"""
        
        version_info = {
            'model_name': model_name,
            'version': version,
            'model_path': model_path,
            'registered_at': datetime.now(),
            'metadata': metadata,
            'status': 'registered'
        }
        
        key = f"{model_name}:{version}"
        self.versions[key] = version_info
        
        self.logger.info(f"Registered model {key}")
        return version_info
    
    def get_model_version(
        self,
        model_name: str,
        version: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """Get specific model version or latest"""
        
        if version:
            key = f"{model_name}:{version}"
            return self.versions.get(key)
        
        # Get latest version
        model_versions = [
            v for k, v in self.versions.items()
            if k.startswith(f"{model_name}:")
        ]
        
        if not model_versions:
            return None
        
        return max(model_versions, key=lambda x: x['registered_at'])
    
    def list_versions(self, model_name: str) -> List[Dict[str, Any]]:
        """List all versions of a model"""
        return [
            v for k, v in self.versions.items()
            if k.startswith(f"{model_name}:")
        ]
    
    def promote_version(
        self,
        model_name: str,
        version: str,
        stage: str  # 'staging', 'production'
    ):
        """Promote model version to different stage"""
        key = f"{model_name}:{version}"
        
        if key in self.versions:
            self.versions[key]['stage'] = stage
            self.versions[key]['promoted_at'] = datetime.now()
            self.logger.info(f"Promoted {key} to {stage}")


# ============================================================================
# 5. EXPERIMENT TRACKER
# ============================================================================

class ExperimentTracker:
    """Track ML experiments and results"""
    
    def __init__(self, experiment_name: str):
        self.experiment_name = experiment_name
        self.runs = []
        self.logger = logging.getLogger(__name__)
    
    def start_run(
        self,
        run_name: str,
        parameters: Dict[str, Any]
    ) -> str:
        """Start a new experiment run"""
        
        run_id = f"{run_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        run_info = {
            'run_id': run_id,
            'run_name': run_name,
            'experiment_name': self.experiment_name,
            'parameters': parameters,
            'metrics': {},
            'artifacts': [],
            'started_at': datetime.now(),
            'status': 'running'
        }
        
        self.runs.append(run_info)
        self.logger.info(f"Started run: {run_id}")
        
        return run_id
    
    def log_metrics(
        self,
        run_id: str,
        metrics: Dict[str, float],
        step: Optional[int] = None
    ):
        """Log metrics for a run"""
        run = self._get_run(run_id)
        
        if not run:
            self.logger.error(f"Run {run_id} not found")
            return
        
        if step is not None:
            if 'metrics_history' not in run:
                run['metrics_history'] = []
            run['metrics_history'].append({'step': step, 'metrics': metrics})
        else:
            run['metrics'].update(metrics)
    
    def log_artifact(
        self,
        run_id: str,
        artifact_path: str,
        artifact_type: str = "model"
    ):
        """Log artifact for a run"""
        run = self._get_run(run_id)
        
        if not run:
            return
        
        run['artifacts'].append({
            'path': artifact_path,
            'type': artifact_type,
            'logged_at': datetime.now()
        })
    
    def end_run(self, run_id: str, status: str = "completed"):
        """End an experiment run"""
        run = self._get_run(run_id)
        
        if not run:
            return
        
        run['status'] = status
        run['ended_at'] = datetime.now()
        run['duration'] = (run['ended_at'] - run['started_at']).total_seconds()
        
        self.logger.info(f"Ended run: {run_id} with status {status}")
    
    def _get_run(self, run_id: str) -> Optional[Dict[str, Any]]:
        """Get run by ID"""
        for run in self.runs:
            if run['run_id'] == run_id:
                return run
        return None
    
    def get_best_run(self, metric: str, mode: str = "max") -> Optional[Dict[str, Any]]:
        """Get best run based on metric"""
        completed_runs = [r for r in self.runs if r['status'] == 'completed']
        
        if not completed_runs:
            return None
        
        valid_runs = [r for r in completed_runs if metric in r['metrics']]
        
        if not valid_runs:
            return None
        
        if mode == "max":
            return max(valid_runs, key=lambda x: x['metrics'][metric])
        else:
            return min(valid_runs, key=lambda x: x['metrics'][metric])


# ============================================================================
# 6. DATA QUALITY CHECKER
# ============================================================================

class DataQualityChecker:
    """Check data quality for incoming requests"""
    
    def __init__(self, schema: Dict[str, Any]):
        self.schema = schema
        self.logger = logging.getLogger(__name__)
    
    def validate(self, data: pd.DataFrame) -> Tuple[bool, List[str]]:
        """Validate data against schema"""
        errors = []
        
        # Check required columns
        required_cols = self.schema.get('required_columns', [])
        missing_cols = set(required_cols) - set(data.columns)
        if missing_cols:
            errors.append(f"Missing required columns: {missing_cols}")
        
        # Check data types
        for col, expected_type in self.schema.get('column_types', {}).items():
            if col in data.columns:
                if not self._check_type(data[col], expected_type):
                    errors.append(f"Column {col} has incorrect type")
        
        # Check value ranges
        for col, range_config in self.schema.get('value_ranges', {}).items():
            if col in data.columns:
                if 'min' in range_config:
                    if (data[col] < range_config['min']).any():
                        errors.append(f"Column {col} has values below minimum")
                if 'max' in range_config:
                    if (data[col] > range_config['max']).any():
                        errors.append(f"Column {col} has values above maximum")
        
        # Check null values
        for col in self.schema.get('non_null_columns', []):
            if col in data.columns:
                if data[col].isnull().any():
                    errors.append(f"Column {col} contains null values")
        
        is_valid = len(errors) == 0
        return is_valid, errors
    
    def _check_type(self, series: pd.Series, expected_type: str) -> bool:
        """Check if series matches expected type"""
        type_mapping = {
            'int': np.integer,
            'float': np.floating,
            'string': object,
            'bool': bool
        }
        
        expected_np_type = type_mapping.get(expected_type)
        if expected_np_type:
            return np.issubdtype(series.dtype, expected_np_type)
        return True


# ============================================================================
# 7. USAGE EXAMPLE
# ============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # Example: Drift Detection
    baseline = pd.DataFrame({
        'feature1': np.random.normal(0, 1, 1000),
        'feature2': np.random.normal(5, 2, 1000)
    })
    
    current = pd.DataFrame({
        'feature1': np.random.normal(0.5, 1.2, 1000),  # Shifted
        'feature2': np.random.normal(5, 2, 1000)
    })
    
    drift_detector = DriftDetector(baseline)
    drift_results = drift_detector.detect_feature_drift(current, threshold=0.1)
    print("Drift Detection Results:")
    for feature, result in drift_results.items():
        print(f"  {feature}: PSI={result['psi']:.4f}, Drift={result['has_drift']}")
    
    # Example: Metrics Tracking
    tracker = MetricsTracker()
    tracker.log_metrics("endpoint-123", {
        'latency_ms': 150,
        'error_rate': 0.01,
        'cpu_usage': 45.2
    })
    
    aggregates = tracker.calculate_aggregates("endpoint-123", window="1h")
    print("\nMetrics Aggregates:", aggregates)
    
    # Example: Alert Management
    alert_manager = AlertManager(notification_channels=['email', 'slack'])
    
    alerts = alert_manager.check_alerts(
        metrics={'latency_ms': 1500, 'error_rate': 0.08},
        alert_configs=[
            {
                'name': 'high_latency',
                'metric': 'latency_ms',
                'condition': '>',
                'threshold': 1000,
                'severity': 'warning',
                'channels': ['email']
            },
            {
                'name': 'high_error_rate',
                'metric': 'error_rate',
                'condition': '>',
                'threshold': 0.05,
                'severity': 'critical',
                'channels': ['email', 'slack']
            }
        ]
    )
    
    print(f"\nTriggered {len(alerts)} alerts")
    for alert in alerts:
        print(f"  - {alert.name}: {alert.message}")
